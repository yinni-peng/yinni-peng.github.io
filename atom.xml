<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Luckyye blog</title>
  
  <subtitle>pursue the truth and persist in it.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-14T08:51:00.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>叶鹏</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Anomaly Detection-高维数据降维聚类后选择统计分析算法有感</title>
    <link href="http://yoursite.com/2019/02/14/dimensionality-reduction-clustering-analysis/"/>
    <id>http://yoursite.com/2019/02/14/dimensionality-reduction-clustering-analysis/</id>
    <published>2019-02-14T06:10:38.000Z</published>
    <updated>2019-02-14T08:51:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>摘要:<br>这段时间，针对Anomaly detection, 陆陆续续接触了很多<strong>clustering</strong>和<strong>outlier detection</strong>相关的algorithms。</p><ul><li>降维方面：<span style="border-bottom:1px dotted #0a6296;">PCA, T-SNE, AutoEncoder</span></li><li>聚类分析：<span style="border-bottom:1px dotted #0a6296;">K-means, DBSCAN</span></li><li>距离密度筛选：<span style="border-bottom:1px dotted #0a6296;">Mahalanobis distance, Euclidean distance, local outlier factor</span></li><li>异常检测算法：<span style="border-bottom:1px dotted #0a6296;">针对此篇文章只谈AutoEncoder, 诸如iforest, oc-svm等均已实现过，后面有空会整理上来。</span>。</li></ul><p>如果只是为了anomaly detection, 其实做起来也很简单，但笔者为了优化AutoEncoder的model，经过各种调参（过拟合、学习率、数据规模、batch-size等）的无济于事之后，将工作重心放在数据样本的筛选上，经历过换各种算法和方法组合的心力交瘁之后，还是有点成效，遂将此番过程记录下来，梳理一遍选择算法时的思路和具体场景的算法怎样选择，以便自己日后回顾。确实大量的时间都用在为了一份好的数据，去尝试各种算法。故对机器学习的一句箴言深表认同：</p><blockquote><p>好的数据永远大于一个好的算法!</p></blockquote><p>好了，以上多半废话，下面进入正文。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘要:&lt;br&gt;这段时间，针对Anomaly detection, 陆陆续续接触了很多&lt;strong&gt;clustering&lt;/strong&gt;和&lt;strong&gt;outlier detection&lt;/strong&gt;相关的algorithms。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;降维方面：
      
    
    </summary>
    
      <category term="anomaly detection" scheme="http://yoursite.com/categories/anomaly-detection/"/>
    
    
      <category term="anomaly detection" scheme="http://yoursite.com/tags/anomaly-detection/"/>
    
      <category term="dimensionality reduction" scheme="http://yoursite.com/tags/dimensionality-reduction/"/>
    
      <category term="clustering algorithm" scheme="http://yoursite.com/tags/clustering-algorithm/"/>
    
      <category term="pca" scheme="http://yoursite.com/tags/pca/"/>
    
      <category term="dbscan" scheme="http://yoursite.com/tags/dbscan/"/>
    
      <category term="auto-encoder" scheme="http://yoursite.com/tags/auto-encoder/"/>
    
      <category term="k-means" scheme="http://yoursite.com/tags/k-means/"/>
    
      <category term="euclidean distance" scheme="http://yoursite.com/tags/euclidean-distance/"/>
    
      <category term="mahalanobis distance" scheme="http://yoursite.com/tags/mahalanobis-distance/"/>
    
      <category term="local outlier factor" scheme="http://yoursite.com/tags/local-outlier-factor/"/>
    
  </entry>
  
  <entry>
    <title>PCA-MD与AutoEncoder两种方式的异常检测和状态监控</title>
    <link href="http://yoursite.com/2019/02/13/anomaly-detection-pca-md-autoencoder/"/>
    <id>http://yoursite.com/2019/02/13/anomaly-detection-pca-md-autoencoder/</id>
    <published>2019-02-13T09:23:32.000Z</published>
    <updated>2019-02-14T02:06:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文摘录并翻译了<a href="https://towardsdatascience.com/" target="_blank" rel="noopener">Towards Data Science</a>下一篇关于异常检测方法的部分内容，并结合<a href="http://www.csuldw.com/" target="_blank" rel="noopener">D.W’s Notes</a>博客的部分翻译，具体地址如下：</p><blockquote><p><a href="https://towardsdatascience.com/how-to-use-machine-learning-for-anomaly-detection-and-condition-monitoring-6742f82900d7" target="_blank" rel="noopener">How to use machine learning for anomaly detection and condition monitoring – Towards Data Science</a><br><a href="http://www.csuldw.com/2019/01/09/2019-01-08-how-to-use-machine-learning-for-anomaly-detection-and-condition-monitoring/" target="_blank" rel="noopener">如何利用机器学习进行异常检测和状态监测 – 刘帝伟的博客 D.W’s Notes</a></p></blockquote><p>摘要：</p><blockquote><p>看完这篇文章，对最近正在尝试anomaly detection的笔者有不小收获，笔者最近尝试了PCA、K-means+Euclidean distance、Auto Encoder等方法，在K-means+ED方法上正因为效果不理想而转到PCA+ED，随即看到了PCA+MD的异常检测解决方案，很是开心。遂翻译记录部分关于PCA+MD的原文，关于Auto Encoder部分，自己也算是比较熟悉了，也一直在用，仅做简单记录。对blog原文及D.W的翻译表示感谢！</p></blockquote><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><p>异常检测（或异常值检测）是对稀有物品、事件或观察结果的识别，这些物品、事件或观察结果与大多数数据存在显著差异，从而引起疑虑。通常，异常数据可以关联到某种问题或罕见事件，例如银行欺诈、医疗问题、结构缺陷、故障设备等。这种连接能够有趣地找出哪些数据点可以被视为异常，因为从业务角度来看，识别这些事件通常非常有趣。</p><p>关键目标：如何识别数据点是正常或是异常？简单的情况下，如下图所示，数据可视化可以比较直观的展示出来。</p><h2 id="Technical-section"><a href="#Technical-section" class="headerlink" title="Technical section"></a>Technical section</h2><p>在不涉及更多技术方面的情况下，很难涵盖机器学习和异常检测统计分析的主题。我会避免太深入讲解理论背景（但会提供一些链接）。如果您对机器学习和统计分析的实际应用更感兴趣，例如状态监控，可跳到“状态监控用例”部分。</p><h3 id="Approach-1-Multivariate-statistical-analysis"><a href="#Approach-1-Multivariate-statistical-analysis" class="headerlink" title="Approach 1: Multivariate statistical analysis"></a>Approach 1: Multivariate statistical analysis</h3><h4 id="Dimensionality-reduction-using-principal-component-analysis-PCA"><a href="#Dimensionality-reduction-using-principal-component-analysis-PCA" class="headerlink" title="Dimensionality reduction using principal component analysis: PCA"></a>Dimensionality reduction using principal component analysis: PCA</h4><p>处理高维数据通常具有挑战性，因此有几种方法可以减少变量的数量（降维）。其中一种技术就是主要成分分析（PCA），它将数据线性映射到低维空间，使数据在低维空间中的方差最大化。在实际应用中，需要构造了数据的协方差矩阵，并计算该矩阵的特征向量。对应于最大特征值（主分量）的特征向量可以用来重建原始数据方差较大的一部分。原来的特征空间已经减少（有一些数据丢失，但会保留最重要的方差）到几个特征向量所跨越的空间。</p><h4 id="Multivariate-anomaly-detection"><a href="#Multivariate-anomaly-detection" class="headerlink" title="Multivariate anomaly detection"></a>Multivariate anomaly detection</h4><p>如上所述，为了在处理一个或两个变量时识别异常，数据可视化通常是一个好的起点。然而，当将此扩展到高维数据（在实际应用中经常是这样）时，这种方法变得越来越困难。幸运的是，多元统计数据起到了作用。</p><p>当处理一组数据点时，它们通常具有一定的分布（例如高斯分布）。为了更定量地检测异常，我们首先从数据点计算概率分布p(x)。然后，当一个新的例子x出现时，我们将p(x)与阈值r进行比较。如果p(x) &lt; r，则将其视为异常。这是因为正常的例子倾向于有一个大的p(x)，而异常的例子倾向于有一个小的p(x)。</p><p>在状态监测的背景下，这很有趣，因为异常可以告诉我们被监测设备的“健康状态”：当设备接近故障时生成的数据，或次优操作，通常与“健康”设备的数据分布不同。</p><h4 id="The-Mahalanobis-distance"><a href="#The-Mahalanobis-distance" class="headerlink" title="The Mahalanobis distance"></a>The Mahalanobis distance</h4><p>如上所述，讨论数据点属于哪种分布的问题。第一步是找到采样点的质心或质心。直观地说，所讨论的点离这个质心越近，它就越有可能属于这个集合。然而，我们还需要知道集合是分布在大范围还是小范围，这样我们就可以决定距中心的给定距离是否值得注意。简单的方法是估计样本点到质心距离的标准偏差。通过将其插入正态分布，我们可以得出数据点属于同一分布的概率。</p><p>上述方法的缺点是，我们假设采样点以球形方式分布在质量中心周围。如果分布确定为非球形，例如椭球形，那么我们期望测试点属于集合的概率不仅取决于与质心的距离，而且取决于方向。在椭球体具有短轴的方向上，试验点必须更近，而在轴较长的方向上，试验点可以远离中心。在数学基础上，通过计算样本的协方差矩阵，可以估计出最能代表集合概率分布的椭球。Mahalanobis距离（md）是测试点到质量中心的距离除以试验点方向上的椭球宽度。</p><p>为了使用MD将测试点分类为属于n个类中的一个类，首先要估计每个类的协方差矩阵，通常基于已知属于每个类的样本。在我们的例子中，由于我们只对“正常”与“异常”进行分类感兴趣，因此我们使用只包含正常操作条件的训练数据来计算协方差矩阵。然后，给出一个测试样本，将MD计算为“正常”类，如果距离超过某个阈值，则将测试点分类为“异常”。</p><p>注意：使用MD意味着可以通过均值和协方差矩阵-进行推断，这只是正态分布的一个性质。在我们的例子中，不一定满足这个标准，因为输入变量可能不是正态分布的。不过我们还是可以试试，看看效果如何！</p><h3 id="Artificial-Neural-Network-Auto-Encoder"><a href="#Artificial-Neural-Network-Auto-Encoder" class="headerlink" title="Artificial Neural Network - Auto Encoder"></a>Artificial Neural Network - Auto Encoder</h3><p><strong>Autoencoder networks</strong><br>第二种方法是使用自编码器神经网络。这是基于与上述统计分析相似的原则，但有一些细微的差异。</p><p>自编码器是一种人工神经网络，用于无监督地学习有效的数据编码。自动编码器的目的是学习一组数据的表示（编码），通常用于降维。与原边一起，学习重构边，其中自动编码器尝试从简约编码生成尽可能接近其原始输入的表示。</p><p>从结构上讲，AutoEncoder最简单的形式是一个前馈的、非重复性的神经网络，与许多单层感知器非常相似，多层感知器（MLP）具有一个输入层、一个输出层和一个或多个连接多层感知器的隐藏层-，但输出层具有相同数量的节点。作为输入层，目的是重建自己的输入。</p><blockquote><p>Auto Encoder神经网络示意图如下：</p></blockquote><p><img src="http://pmunz023v.bkt.clouddn.com/auto-encoder.jpeg" alt="Figure 1: Autoencoder network"><br>在异常检测和状态监测的背景下，基本思想是使用自动编码器网络将传感器读数“压缩”到较低的尺寸表示，从而捕获各种变量之间的相关性和相互作用。（基本上与PCA模型的原理相同，但这里我们也考虑了变量之间的非线性相互作用）。</p><p>然后，自动编码器网络接受表示“正常”操作状态的数据的训练，其目标是首先压缩然后重建输入变量。在降维过程中，网络学习各种变量之间的相互作用，并能在输出端将它们重新构造回原来的变量。主要想法是，随着被监测设备的退化，这将影响变量之间的相互作用（例如温度、压力、振动等的变化）。当这种情况发生时，人们将开始看到网络重新构造输入变量的错误增加。通过监测重建误差，人们可以得到被监测设备的“健康”指示，因为这种误差会随着设备的退化而增加。与第一种使用Mahalanobis距离的方法类似，我们这里使用重建误差的概率分布来确定数据点是正常的还是异常的。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>仅摘录这么多内容，其他部分主要最对工业示例进行对比，本文的重点在于基于统计分析和神经网络两种方式的了解，近期会hava a try, 有价值的结论后期会更新至文末。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文摘录并翻译了&lt;a href=&quot;https://towardsdatascience.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Towards Data Science&lt;/a&gt;下一篇关于异常检测方法的部分内容，并结合&lt;a href=&quot;htt
      
    
    </summary>
    
      <category term="anomaly detection" scheme="http://yoursite.com/categories/anomaly-detection/"/>
    
    
      <category term="anomaly detection" scheme="http://yoursite.com/tags/anomaly-detection/"/>
    
      <category term="dimensionality reduction" scheme="http://yoursite.com/tags/dimensionality-reduction/"/>
    
      <category term="pca" scheme="http://yoursite.com/tags/pca/"/>
    
      <category term="auto-encoder" scheme="http://yoursite.com/tags/auto-encoder/"/>
    
      <category term="mechine learning" scheme="http://yoursite.com/tags/mechine-learning/"/>
    
      <category term="Mahalanobis distance" scheme="http://yoursite.com/tags/Mahalanobis-distance/"/>
    
  </entry>
  
  <entry>
    <title>【转载】机器学习聚类算法—k-means聚类</title>
    <link href="http://yoursite.com/2019/02/13/clustering-algorithm-kmeans/"/>
    <id>http://yoursite.com/2019/02/13/clustering-algorithm-kmeans/</id>
    <published>2019-02-13T03:02:20.000Z</published>
    <updated>2019-02-13T06:20:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文地址： <a href="http://www.csuldw.com/2015/06/03/2015-06-03-ml-algorithm-K-means/" target="_blank" rel="noopener">机器学习算法—K-means聚类</a><br>K-means代码实现：<a href="https://github.com/csuldw/MachineLearning/tree/master/Kmeans" target="_blank" rel="noopener">https://github.com/csuldw/MachineLearning/tree/master/Kmeans</a></p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>k-Means算法是一种聚类算法，它是一种无监督学习算法，目的是将相似的对象归到同一个簇中。簇内的对象越相似，聚类的效果就越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。其产生的结果和分类相同，而只是类别没有预先定义。</p><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>设计的目的：<strong>使各个样本与所在簇的质心的均值的误差平方和达到最小</strong>（这也是评价K-means算法最后聚类效果的评价标准，简称SSE）。</p><blockquote><p>计算公式如下：</p></blockquote><p><img src="http://pmunz023v.bkt.clouddn.com/kmeans-sse.gif" alt=""></p><h4 id="k-均值聚类"><a href="#k-均值聚类" class="headerlink" title="k-均值聚类"></a>k-均值聚类</h4><ul><li>优点：容易实现</li><li>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</li><li>适合数据类型：数值型数据</li></ul><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol><li>创建k个点作为k个簇的起始质心（经常随机选择）。</li><li>分别计算剩下的元素到k个簇中心的相异度（距离），将这些元素分别划归到相异度最低的簇。</li><li>根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均值。 </li><li>将D中全部元素按照新的中心重新聚类。 </li><li>重复第4步，直到聚类结果不再变化。</li><li>最后，输出聚类结果</li></ol><h4 id="算法伪代码实现"><a href="#算法伪代码实现" class="headerlink" title="算法伪代码实现"></a>算法伪代码实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建k个点作为K个簇的起始质心（经常随机选择）</span><br><span class="line">当任意一个点的蔟分配结果发生变化时（初始化为True）</span><br><span class="line">对数据集中的每个数据点，重新分配质心</span><br><span class="line">对每个质心</span><br><span class="line">计算质心到数据点之间的距离</span><br><span class="line">将数据点分配到距其最近的蔟</span><br><span class="line">对每个蔟，计算蔟中所有点的均值并将均值作为新的质心</span><br></pre></td></tr></table></figure><h4 id="示例实现"><a href="#示例实现" class="headerlink" title="示例实现"></a>示例实现</h4><p>因为我们用到的是数值类型数据，这里需要编写一个加载数据集的函数，其返回值是一个矩阵。下面代码应写在一个py文件里，我这里写在kMeans.py文件中。</p><p>首先引入相关的头文件：numpy<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure></p><h4 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集文件，没有返回类标号的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    openfile = open(fileName)    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> openfile.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        floatLine = map(float,curLine)</span><br><span class="line">        dataMat.append(floatLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br></pre></td></tr></table></figure><p>因为在k均值算法中要计算点到质心的距离，所以这里将距离计算写成一个函数，计算欧几里得距离公式：<br>$$d=\sqrt{(x_1-z_1)^2+…+(x_n-z_n)^2}$$</p><p>欧几里得距离计算公式如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个向量的欧氏距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA,vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA-vecB,<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">numpy.sqrt(numpy.sum(numpy.square(vec1 - vec2))</span><br></pre></td></tr></table></figure></p><h4 id="接下来初始化k个蔟的质心函数centroid"><a href="#接下来初始化k个蔟的质心函数centroid" class="headerlink" title="接下来初始化k个蔟的质心函数centroid"></a>接下来初始化k个蔟的质心函数centroid</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传入的数据时numpy的矩阵格式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataMat, k)</span>:</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))  </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        minJ = min(dataMat[:,j]) <span class="comment"># 找出矩阵dataMat第j列最小值</span></span><br><span class="line">        rangeJ = float(max(dataMat[:,j]) - minJ) <span class="comment">#计算第j列最大值和最小值的差</span></span><br><span class="line">        <span class="comment">#赋予一个随机质心，它的值在整个数据集的边界之内</span></span><br><span class="line">        centroids[:,j] = minJ + rangeJ * random.rand(k,<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids <span class="comment">#返回一个随机的质心矩阵</span></span><br></pre></td></tr></table></figure><h4 id="K-means算法实现"><a href="#K-means算法实现" class="headerlink" title="K-means算法实现"></a>K-means算法实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回的第一个变量时质心，第二个是各个簇的分布情况</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataMat,k,distE = distEclud , createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataMat)[<span class="number">0</span>]    <span class="comment"># 获得行数m</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment"># 初试化一个矩阵，用来记录簇索引和存储误差                               </span></span><br><span class="line">    centroids = createCent(dataMat,k) <span class="comment"># 随机的得到一个质心矩阵蔟</span></span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):    <span class="comment">#对每个数据点寻找最近的质心</span></span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment"># 遍历质心蔟，寻找最近的质心    </span></span><br><span class="line">                distJ1 = distE(centroids[j,:],dataMat[i,:]) <span class="comment">#计算数据点和质心的欧式距离</span></span><br><span class="line">                <span class="keyword">if</span> distJ1 &lt; minDist: </span><br><span class="line">                    minDist = distJ1</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex:</span><br><span class="line">                clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i,:] = minIndex, minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line">        <span class="keyword">for</span> ci <span class="keyword">in</span> range(k):   <span class="comment">#更新质心，将每个族中的点的均值作为质心</span></span><br><span class="line">            index_all = clusterAssment[:,<span class="number">0</span>].A   <span class="comment">#取出样本所属簇的索引值</span></span><br><span class="line">            value = nonzero(index_all==ci)    <span class="comment">#取出所有属于第ci个簇的索引值</span></span><br><span class="line">            sampleInClust = dataMat[value[<span class="number">0</span>]]     <span class="comment">#取出属于第i个簇的所有样本点</span></span><br><span class="line">            centroids[cent,:] = mean(sampleInClust, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure><p>虽然K-Means算法原理简单，但是也有自身的缺陷：</p><ul><li>首先，聚类的簇数K值需要事先给定，但在实际中这个 K 值的选定是非常难以估计的，很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。</li><li>Kmeans需要人为地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果，不能保证Ｋ-Means算法收敛于全局最优解。<ul><li>针对此问题，在K-Means的基础上提出了二分K-means算法。该算法首先将所有点看做是一个簇，然后一分为二，找到最小SSE的聚类质心。接着选择其中一个簇继续一分为二，此处哪一个簇需要根据划分后的SSE值来判断。</li></ul></li><li>对离群点敏感。</li><li>结果不稳定 （受输入顺序影响）。</li><li>时间复杂度高O(nkt)，其中n是对象总数，k是簇数，t是迭代次数。</li></ul><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>在控制台调用上述函数，执行下列命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataMat = mat(loadDataSet(<span class="string">'testSet.txt'</span>))</span><br><span class="line">kMeans(dataMat, <span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><p>运行上面代码后，可以看到print处输出的结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[-3.66087851  2.30869657]</span><br><span class="line"> [ 3.24377288  3.04700412]</span><br><span class="line"> [ 2.52577861 -3.12485493]</span><br><span class="line"> [-2.79672694  3.19201596]]</span><br><span class="line">[[-3.78710372 -1.66790611]</span><br><span class="line"> [ 2.6265299   3.10868015]</span><br><span class="line"> [ 1.62908469 -2.92689085]</span><br><span class="line"> [-2.18799937  3.01824781]]</span><br><span class="line">[[-3.53973889 -2.89384326]</span><br><span class="line"> [ 2.6265299   3.10868015]</span><br><span class="line"> [ 2.65077367 -2.79019029]</span><br><span class="line"> [-2.46154315  2.78737555]]</span><br></pre></td></tr></table></figure></p><p>测试的时候取的k值为4，即簇的个数为4，所以经过3次迭代之后K-均值算法就收敛了。质心会保存在第一个返回值中，第二个是每个点的簇分布情况。k=4是，聚类结果如下:<br><img src="http://pmunz023v.bkt.clouddn.com/k-means-res.png" alt=""><br>附数据集：<a href="https://github.com/csuldw/MachineLearning/blob/master/Kmeans/data/testSet.txt" target="_blank" rel="noopener">https://github.com/csuldw/MachineLearning/blob/master/Kmeans/data/testSet.txt</a></p><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><h4 id="K-Means算法K值如何选择？"><a href="#K-Means算法K值如何选择？" class="headerlink" title="K-Means算法K值如何选择？"></a>K-Means算法K值如何选择？</h4><p>《大数据》中提到：给定一个合适的类簇指标，比如平均半径或直径，只要我们假设的类簇的数目等于或者高于真实的类簇的数目时，该指标上升会很缓慢，而一旦试图得到少于真实数目的类簇时，该指标会急剧上升。</p><ul><li>簇的直径是指簇内任意两点之间的最大距离。</li><li>簇的半径是指簇内所有点到簇中心距离的最大值。</li></ul><h4 id="如何优化K-Means算法搜索的时间复杂度？"><a href="#如何优化K-Means算法搜索的时间复杂度？" class="headerlink" title="如何优化K-Means算法搜索的时间复杂度？"></a>如何优化K-Means算法搜索的时间复杂度？</h4><p>可以使用K-D树来缩短最近邻的搜索时间（NN算法都可以使用K-D树来优化时间复杂度）。</p><h4 id="如何确定K个簇的初始质心？"><a href="#如何确定K个簇的初始质心？" class="headerlink" title="如何确定K个簇的初始质心？"></a>如何确定K个簇的初始质心？</h4><p>1) 选择批次距离尽可能远的K个点</p><p>首先随机选择一个点作为第一个初始类簇中心点，然后选择距离该点最远的那个点作为第二个初始类簇中心点，然后再选择距离前两个点的最近距离最大的点作为第三个初始类簇的中心点，以此类推，直至选出K个初始类簇中心点。</p><p>2) 选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为KMeans算法初始类簇中心点。</p><p>聚类扩展：密度聚类、层次聚类。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/clustering.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/clustering.pdf</a></li><li><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html" target="_blank" rel="noopener">http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html</a></li><li><a href="http://www.csuldw.com/2015/06/03/2015-06-03-ml-algorithm-K-means/" target="_blank" rel="noopener">http://www.csuldw.com/2015/06/03/2015-06-03-ml-algorithm-K-means/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原文地址： &lt;a href=&quot;http://www.csuldw.com/2015/06/03/2015-06-03-ml-algorithm-K-means/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;机器学习算法—K-means聚类&lt;/a&gt;&lt;br&gt;
      
    
    </summary>
    
      <category term="clustering algorithm" scheme="http://yoursite.com/categories/clustering-algorithm/"/>
    
    
      <category term="clustering algorithm" scheme="http://yoursite.com/tags/clustering-algorithm/"/>
    
      <category term="k-means" scheme="http://yoursite.com/tags/k-means/"/>
    
      <category term="meachine learning" scheme="http://yoursite.com/tags/meachine-learning/"/>
    
  </entry>
  
  <entry>
    <title>关于重写blog的一些说明</title>
    <link href="http://yoursite.com/2019/02/12/first-note/"/>
    <id>http://yoursite.com/2019/02/12/first-note/</id>
    <published>2019-02-12T04:27:39.000Z</published>
    <updated>2019-02-13T03:27:36.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="关于之前的blog"><a href="#关于之前的blog" class="headerlink" title="关于之前的blog"></a>关于之前的blog</h4><p>2016-2018大学期间，零散记录了约80篇blog，数量虽不少，但有较强阅读价值的屈指可数；另之前保存blog的source<br>md文件在老家的移动硬盘中，也懒得再migrate过来。遂下定决心重新开始纪录。</p><h4 id="关于新blog的内容"><a href="#关于新blog的内容" class="headerlink" title="关于新blog的内容"></a>关于新blog的内容</h4><p>新blog的内容会重点留下自己mechine learning学习路途中的理解和感悟、别人有价值的文章阅读、项目或比赛中用到的一些算法及思路的解释、及个人心得等。</p><p>遂留此篇blog为证！</p><blockquote><p>2019-02-12 叶鹏</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;关于之前的blog&quot;&gt;&lt;a href=&quot;#关于之前的blog&quot; class=&quot;headerlink&quot; title=&quot;关于之前的blog&quot;&gt;&lt;/a&gt;关于之前的blog&lt;/h4&gt;&lt;p&gt;2016-2018大学期间，零散记录了约80篇blog，数量虽不少，但有较强阅读价值
      
    
    </summary>
    
      <category term="personal notes" scheme="http://yoursite.com/categories/personal-notes/"/>
    
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
</feed>
